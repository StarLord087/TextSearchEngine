{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import string \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "from num2words import num2words\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import heapq\n",
    "import re\n",
    "import pandas as pd\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dir = 'C:\\\\Users\\\\shekh\\\\Desktop\\\\Shekhar\\\\IR\\\\Assignment 2\\\\stories'\n",
    "folders = []\n",
    "files = []\n",
    "docs = []\n",
    "titles = []\n",
    "names = []\n",
    "titles = []\n",
    "\n",
    "idtoname = {}\n",
    "nametoid = {}\n",
    "\n",
    "folders.append(my_dir)\n",
    "for folder in os.listdir(my_dir):\n",
    "    newpath = my_dir +'\\\\'+ folder\n",
    "    #print(newpath)\n",
    "    if os.path.isdir(newpath):\n",
    "        folders.append(newpath)\n",
    "        \n",
    "#print(folders)\n",
    "\n",
    "for folder in folders:\n",
    "    #print(folder)\n",
    "    for file in os.listdir(folder):\n",
    "        #print(file)\n",
    "        if(file == \"index.html\"):\n",
    "            file = open(folder+'\\\\'+file, 'r')\n",
    "            text = file.read().strip()\n",
    "            file.close()\n",
    "            names.extend(re.findall('><A HREF=\"(.*)\">', text))\n",
    "            titles.extend(re.findall(r'<BR><TD> (.*)\\n', text))\n",
    "\n",
    "names = names[2:]\n",
    "\n",
    "for name in names:\n",
    "    for folder in folders:\n",
    "        for file in os.listdir(folder):\n",
    "            if file == name:\n",
    "                f = open(folder+ '\\\\'+file, 'r', errors = 'ignore')\n",
    "                doc = f.read() \n",
    "                docs.append(doc)\n",
    "                f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # inputstr = \"The 5,biggest countries by the the the the the.0.7..   population °°°°°°°°°°°°°°°°°°°°°°°°°°°°°furri in 2017 are China, India, time-travel, United States, Indonesia, and Brazil.[!#$%&\"\"()*+,-./:;<=>?@[\\]^_`{|}~]:\"\n",
    "        # inputstr2 = \"the Box A contains 3 red and 5 American     white balls, while Box B contains 4 red and 2 blue balls.[!#$%&\"\"()*+,-./:;<=>?@[\\]^_`{|}~]:\"\n",
    "        # inputstr3 = \"The online encyclopedia project brazil Wikipedia is the most popular united states wiki-based website, and is one of the most widely viewed sites in the world, having been ranked in the top ten since 2007.\"\n",
    "        # inputstr4 = \"the Casey Owen Neistat is an American 5 YouTube personality, india india filmmaker, vlogger and co-founder of the multimedia company Beme, which was later acquired by CNN. In 2018, he founded 368, a creative space for creators to collaborate and influence each other.\"\n",
    "        # inputs = []\n",
    "        # inputs.append(inputstr)\n",
    "        # inputs.append(inputstr2)\n",
    "        # inputs.append(inputstr3)\n",
    "        # inputs.append(inputstr4)\n",
    "        #print(type(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467\n",
      "467\n"
     ]
    }
   ],
   "source": [
    "print(len(titles))\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listtodict(keys, values):\n",
    "    keys = keys.copy()\n",
    "    values = values.copy()\n",
    "    dic = {}\n",
    "    for key in range(len(keys)): \n",
    "        for value in values: \n",
    "            dic[key] = value\n",
    "            values.remove(value)\n",
    "            break  \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_proter_stemmer(string):\n",
    "    for i in range(len(string)):\n",
    "        string[i] = ps.stem(string[i])\n",
    "    return string\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(string):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    data = [w for w in string if not w in stop_words]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_num(string):\n",
    "    for i in range(len(string)):\n",
    "        try:\n",
    "            if(string[i].isnumeric()):\n",
    "                string[i] = num2words(string[i])\n",
    "        except:\n",
    "            continue\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(newdataset):\n",
    "    dataset = newdataset.copy()\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        dataset[i] = dataset[i].lower()\n",
    "        dataset[i] = remove_nonascii(dataset[i])\n",
    "        dataset[i] = dataset[i].translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "        dataset[i] = dataset[i].split()\n",
    "        dataset[i] = remove_stopword(dataset[i])\n",
    "        dataset[i] = convert_num(dataset[i])\n",
    "        dataset[i] = apply_proter_stemmer(dataset[i])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def querypreprocessing(stringdata):\n",
    "    lower = stringdata.lower()\n",
    "    punc = lower.translate(str.maketrans('','',string.punctuation))\n",
    "    spl = punc.split()\n",
    "    removestop = remove_stopword(spl)\n",
    "    removenum = convert_num(removestop)\n",
    "    stemmedquery = apply_proter_stemmer(removenum)\n",
    "    return stemmedquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonascii(string):\n",
    "    stripped = (c for c in string if 0 < ord(c) < 127)\n",
    "    return ''.join(stripped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#d = preprocessing(inputs)\n",
    "#print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Jaccard Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccardcoefficient(data, query):\n",
    "    dataset = set(data)\n",
    "    queryset = set(query)\n",
    "    \n",
    "    unionset = dataset|queryset\n",
    "    intersectionset = dataset&queryset\n",
    "\n",
    "    return len(intersectionset)/len(unionset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    vocab = set()\n",
    "    for doc in data:\n",
    "        for word in doc:\n",
    "            #print(word)\n",
    "            vocab.add(word)\n",
    "    return list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processedsample = preprocessing(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 467/467 [00:29<00:00, 15.96it/s]\n"
     ]
    }
   ],
   "source": [
    "processedata = preprocessing(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = create_vocabulary(processedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacard Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccardretrieval(dataset, query,k):\n",
    "    processedquery = querypreprocessing(query)\n",
    "    \n",
    "    for word in processedquery:\n",
    "        if word not in vocabulary:\n",
    "            processedquery.remove(word)\n",
    "    \n",
    "    jaccard_coeff_list = []\n",
    "    for data in dataset:\n",
    "        jaccard_coeff_list.append(jaccardcoefficient(data, processedquery))\n",
    "    return heapq.nlargest(k, range(len(jaccard_coeff_list)), jaccard_coeff_list.__getitem__), heapq.nlargest(10,jaccard_coeff_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(titles)):\n",
    "    titles[i] = querypreprocessing(titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idtotitle = listtodict(docs, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "idtoname = listtodict(docs, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter Query for Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter query: 100 west by 53 north\n",
      "Enter Value of K: 10\n",
      "ID:  317  NAME:  peace.fun  JACCARD COEFF: 0.02040816326530612\n",
      "ID:  325  NAME:  poem-2.txt  JACCARD COEFF: 0.008333333333333333\n",
      "ID:  384  NAME:  snowmaid.txt  JACCARD COEFF: 0.008264462809917356\n",
      "ID:  438  NAME:  weaver.txt  JACCARD COEFF: 0.0078125\n",
      "ID:  436  NAME:  wall.art  JACCARD COEFF: 0.007692307692307693\n",
      "ID:  330  NAME:  prince.art  JACCARD COEFF: 0.0058823529411764705\n",
      "ID:  381  NAME:  sis  JACCARD COEFF: 0.0057251908396946565\n",
      "ID:  105  NAME:  campfire.txt  JACCARD COEFF: 0.00546448087431694\n",
      "ID:  387  NAME:  socialvikings.txt  JACCARD COEFF: 0.005333333333333333\n",
      "ID:  386  NAME:  social.vikings  JACCARD COEFF: 0.005319148936170213\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter query: \")\n",
    "k = int(input(\"Enter Value of K: \"))\n",
    "result, jaccardcoeff = jaccardretrieval(processedata, query,k)\n",
    "\n",
    "for (i,j) in zip(result, jaccardcoeff):\n",
    "    print(\"ID: \",i,\" NAME: \",idtoname.get(i),\" JACCARD COEFF:\", j)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf1(data, word):\n",
    "    return data.count(word)/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf2(data, word):\n",
    "    if data.count(word) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1+(math.log10(data.count(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(vocab, data, termfreq):\n",
    "    df = pd.DataFrame()\n",
    "    df['vocab'] = vocab\n",
    "    \n",
    "    \n",
    "    tfall = []\n",
    "    \n",
    "    for i in tqdm(range(len(data))):\n",
    "        tf1 = []\n",
    "        for word in vocab:\n",
    "            tf1.append(termfreq(data[i],word))\n",
    "        tfall.append(tf1)\n",
    "    #print(tfall)\n",
    "    \n",
    "    for i in range(len(tfall)):\n",
    "        df.insert(i+1, i, tfall[i])\n",
    "    \n",
    "    df.set_index('vocab', inplace = True)\n",
    "    \n",
    "    nonzero = np.count_nonzero(df,axis = 1) \n",
    "    \n",
    "    idf = []\n",
    "    for i in tqdm(range(len(nonzero))):\n",
    "        #print(nonzero[i])\n",
    "        idf.append(1/(1+(math.log10(df.shape[1]/nonzero[i]))))\n",
    "    \n",
    "    df['idf'] = idf\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf(df1):\n",
    "    df = df1.copy()\n",
    "    df.iloc[:, 0:-1] = df.iloc[:, 0:-1].mul(df.iloc[:, -1], axis=0)\n",
    "    #print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Matching Score Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_score_retrieval(df1, query):\n",
    "    \n",
    "    df = df1.copy()\n",
    "    processedquery = querypreprocessing(query)\n",
    "    \n",
    "    for word in processedquery:\n",
    "        if word not in vocabulary:\n",
    "            processedquery.remove(word)\n",
    "    \n",
    "    df = df.drop(labels = 'idf', axis = 1)\n",
    "    ds = df.loc[processedquery[0]]\n",
    "    \n",
    "    for i in range(1,len(processedquery)):\n",
    "        ds = ds+df.loc[processedquery[i]]\n",
    "    \n",
    "    return ds.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unweighted Dataframe with tf = tf / #of words in doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 467/467 [08:37<00:00,  1.11s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 33137/33137 [00:00<00:00, 244343.78it/s]\n"
     ]
    }
   ],
   "source": [
    "df = create_dataframe(vocabulary, processedata, tf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unweighted Dataframe with tf = 1+log(tf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 467/467 [09:11<00:00,  1.18s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 33137/33137 [00:00<00:00, 474648.77it/s]\n"
     ]
    }
   ],
   "source": [
    "df2 = create_dataframe(vocabulary, processedata, tf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfdataframe = create_tfidf(df)\n",
    "tfidfdataframe2 = create_tfidf(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter query for Q2 unweighted tf = tf / #of words in doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query:50000 variety of flowers\n",
      "Enter value of k: 10\n",
      "ghost\n",
      "narciss.txt\n",
      "day.in.mcdonald\n",
      "mcdonaldl.txt\n",
      "quarter.c9\n",
      "wall.art\n",
      "bulnoopt.txt\n",
      "lgoldbrd.txt\n",
      "ccm.txt\n",
      "fantas.hum\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter Query:\")\n",
    "k = int(input(\"Enter value of k: \"))\n",
    "ser = matching_score_retrieval(tfidfdataframe, query)\n",
    "retdocs = ser.index.tolist()\n",
    "#print(retdocs)\n",
    "#print(idtoname.get(435))\n",
    "for i in range(0,k):\n",
    "    print(idtoname.get(retdocs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter query for Q2 unweighted tf = 1+log(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query:do cindy abyss\n",
      "Enter value of k: 10\n",
      "descent.poe\n",
      "abyss.txt\n",
      "sick-kid.txt\n",
      "empty.txt\n",
      "breaks3.asc\n",
      "forgotte\n",
      "angry_ca.txt\n",
      "robotech\n",
      "enchdup.hum\n",
      "timem.hac\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter Query:\")\n",
    "k = int(input(\"Enter value of k: \"))\n",
    "\n",
    "ser1 = matching_score_retrieval(tfidfdataframe2, query)\n",
    "retdocs1 = ser1.index.tolist()\n",
    "#print(retdocs)\n",
    "#print(idtoname.get(435))\n",
    "for i in range(0,k):\n",
    "    print(idtoname.get(retdocs1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"west\" in idtotitle[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(processedata[1].count(\"get\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_dataframe(vocab, data, heading, termfreq):\n",
    "    df = pd.DataFrame()\n",
    "    df['vocab'] = vocab\n",
    "    titles = heading.copy()\n",
    "    \n",
    "    \n",
    "    tfall = []\n",
    "    \n",
    "    for i in tqdm(range(len(data))):\n",
    "        tf1 = []\n",
    "        for word in vocab:\n",
    "            #print(word)\n",
    "            if(word in idtotitle[i]):\n",
    "                #print(word, i, idtotitle[i])\n",
    "                tf1.append(termfreq(data[i],word)*0.7)\n",
    "            else:\n",
    "                tf1.append(termfreq(data[i],word)*0.3)\n",
    "        tfall.append(tf1)\n",
    "    #print(tfall)\n",
    "    for i in range(len(tfall)):\n",
    "        df.insert(i+1, i, tfall[i])\n",
    "    \n",
    "    df.set_index('vocab', inplace = True)\n",
    "    \n",
    "    nonzero = np.count_nonzero(df,axis = 1) \n",
    "    \n",
    "    idf = []\n",
    "    for i in tqdm(range(len(nonzero))):\n",
    "        #print(nonzero[i])\n",
    "        idf.append(1/(1+(math.log10(df.shape[1]/nonzero[i]))))\n",
    "    \n",
    "    df['idf'] = idf\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted Dataframe with tf = tf / #of words in doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 467/467 [08:37<00:00,  1.11s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 33137/33137 [00:00<00:00, 461208.79it/s]\n"
     ]
    }
   ],
   "source": [
    "weighteddf = create_weighted_dataframe(vocabulary,processedata,titles, tf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted Dataframe with tf = 1+log(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 467/467 [09:15<00:00,  1.19s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 33137/33137 [00:00<00:00, 481535.84it/s]\n"
     ]
    }
   ],
   "source": [
    "weighteddf2 = create_weighted_dataframe(vocabulary, processedata, titles, tf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightedtfidfdataframe = create_tfidf(weighteddf)\n",
    "weightedtfidfdataframe2 = create_tfidf(weighteddf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter query for Q2 weighted with tf = tf / #of words in doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query:do cindy vampyre\n",
      "Enter value of k: 10\n",
      "vampword.txt\n",
      "peace.fun\n",
      "forgotte\n",
      "eyeargon.hum\n",
      "emperor3.txt\n",
      "empnclot.txt\n",
      "empsjowk.txt\n",
      "empty.txt\n",
      "enc\n",
      "encamp01.txt\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter Query:\")\n",
    "k = int(input(\"Enter value of k: \"))\n",
    "\n",
    "ser2 = matching_score_retrieval(weightedtfidfdataframe, query)\n",
    "retdocs2 = ser2.index.tolist()\n",
    "for i in range(0,k):\n",
    "    print(idtoname.get(retdocs2[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter query for Q2 weighted with tf =  1+log(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query:do cindy abyss\n",
      "Enter value of k: 10\n",
      "abyss.txt\n",
      "descent.poe\n",
      "sick-kid.txt\n",
      "empty.txt\n",
      "breaks3.asc\n",
      "forgotte\n",
      "angry_ca.txt\n",
      "robotech\n",
      "enchdup.hum\n",
      "timem.hac\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter Query:\")\n",
    "k = int(input(\"Enter value of k: \"))\n",
    "\n",
    "ser3 = matching_score_retrieval(weightedtfidfdataframe2, query)\n",
    "retdocs3 = ser3.index.tolist()\n",
    "for i in range(0,k):\n",
    "    print(idtoname.get(retdocs3[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Cosine Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2)/(math.sqrt(np.sum(np.square(vec1)))*math.sqrt(np.sum(np.square(vec2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_retrieval(df1, query):\n",
    "    \n",
    "    df = df1.copy()\n",
    "    \n",
    "    processedquery = querypreprocessing(query)\n",
    "    \n",
    "    for word in processedquery:\n",
    "        if word not in vocabulary:\n",
    "            processedquery.remove(word)\n",
    "    \n",
    "    #print(processedquery.count(\"cindi\")/len(processedquery))\n",
    "    queryvector = np.zeros((df.shape[0]))\n",
    "    #print(len(queryvector))\n",
    "    wordvector = df.index.values \n",
    "    \n",
    "    for i in range(len(wordvector)):\n",
    "        for word in processedquery:\n",
    "            if wordvector[i] == word:\n",
    "                #print(wordvector[i], word)\n",
    "                #print(processedquery.count(word)/len(processedquery))\n",
    "                queryvector[i] = processedquery.count(word)/len(processedquery)\n",
    "                #print(queryvector[i],i)\n",
    "        \n",
    "    \n",
    "    idf = df.iloc[:,-1].values \n",
    "    #print(len(idf))\n",
    "    #print(queryvector)\n",
    "    querytfidf = np.multiply(queryvector,idf)\n",
    "    \n",
    "    #for i in querytfidf:\n",
    "     #   if i > 0:\n",
    "      #      print(\"queryvector\",i)\n",
    "    \n",
    "    df = df.drop(labels = 'idf', axis = 1)\n",
    "    #print(df.shape[1])\n",
    "    #print(len(queryvector))\n",
    "    similarity = []\n",
    "    \n",
    "    for i in range(df.shape[1]):\n",
    "        val = tfidfdataframe.iloc[:,i].values\n",
    "        similarity.append(cosine_similarity(querytfidf, val))\n",
    "    \n",
    "    nonzero = []\n",
    "    \n",
    "    for i in similarity:\n",
    "        if i > 0:\n",
    "            nonzero.append(i)\n",
    "    #print(nonzero)\n",
    "    nonzero.sort(reverse=True)\n",
    "    result = []\n",
    "    for i in nonzero:\n",
    "        result.append(similarity.index(i))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter query for Q3 unweighted tf = tf / #of words in doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query:100 animals\n",
      "Enter value of K:10\n",
      "monkking.txt\n",
      "redragon.txt\n",
      "hotline4.txt\n",
      "bran\n",
      "dwar\n",
      "horsdonk.txt\n",
      "ccm.txt\n",
      "fic4\n",
      "clevdonk.txt\n",
      "quarter.c15\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter Query:\")\n",
    "k = int(input(\"Enter value of K:\"))\n",
    "result_cosine = cosine_retrieval(tfidfdataframe, query)\n",
    "for i in range(int(k)):\n",
    "    \n",
    "    print(idtoname[result_cosine[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter query for Q2 unweighted tf = 1+log(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query: 100 west by 53 north\n",
      "Enter Value of k: 10\n",
      "100west.txt\n",
      "arctic.txt\n",
      "bruce-p.txt\n",
      "peace.fun\n",
      "candle.hum\n",
      "dakota.txt\n",
      "history5.txt\n",
      "charlie.txt\n",
      "melissa.txt\n",
      "sis\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter Query: \")\n",
    "k = int(input(\"Enter Value of k: \"))\n",
    "result_cosine2 = cosine_retrieval(tfidfdataframe2, query)\n",
    "for i in range(len(result_cosine2)):\n",
    "    if i == k:\n",
    "        break\n",
    "    print(idtoname[result_cosine2[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter query for Q3 weighted tf = tf / #of words in doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query: 100 animals\n",
      "Enter Value of k10\n",
      "monkking.txt\n",
      "redragon.txt\n",
      "hotline4.txt\n",
      "bran\n",
      "dwar\n",
      "horsdonk.txt\n",
      "ccm.txt\n",
      "fic4\n",
      "clevdonk.txt\n",
      "quarter.c15\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter Query: \")\n",
    "k = int(input(\"Enter Value of k\"))\n",
    "\n",
    "result_weighted_cosine = cosine_retrieval(weightedtfidfdataframe, query)\n",
    "for i in range(len(result_weighted_cosine)):\n",
    "    if i == k:\n",
    "        break\n",
    "    print(idtoname[result_weighted_cosine[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter query for Q3 weighted tf = 1+log(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query: 100 west by 53 north\n",
      "Enter Value of k10\n",
      "100west.txt\n",
      "arctic.txt\n",
      "bruce-p.txt\n",
      "peace.fun\n",
      "candle.hum\n",
      "dakota.txt\n",
      "history5.txt\n",
      "charlie.txt\n",
      "melissa.txt\n",
      "sis\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter Query: \")\n",
    "k = int(input(\"Enter Value of k\"))\n",
    "\n",
    "result_weighted_cosine2 = cosine_retrieval(weightedtfidfdataframe2, query)\n",
    "for i in range(len(result_weighted_cosine2)):\n",
    "    if i == k:\n",
    "        break\n",
    "    print(idtoname[result_weighted_cosine2[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
